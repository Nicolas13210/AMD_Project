{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxTgyqjp3wR9ud/j1OXYyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolas13210/AMD_Project/blob/main/AMDproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms for massive datasets \n",
        "## Project Report\n",
        "Done by Nicolas Audoux"
      ],
      "metadata": {
        "id": "_R7T_6aJL8RE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tTfjrdJ6lvWC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import random\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset"
      ],
      "metadata": {
        "id": "1ysr2RNAxBqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"XXXXXXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXX\"\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset --force\n",
        "!unzip yelp-dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siOfzXe2mnI8",
        "outputId": "8edade56-971c-4531-f028-a5a9bde03ebf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading yelp-dataset.zip to /content\n",
            "100% 4.06G/4.07G [00:40<00:00, 122MB/s]\n",
            "100% 4.07G/4.07G [00:40<00:00, 109MB/s]\n",
            "Archive:  yelp-dataset.zip\n",
            "  inflating: Dataset_User_Agreement.pdf  \n",
            "  inflating: yelp_academic_dataset_business.json  \n",
            "  inflating: yelp_academic_dataset_checkin.json  \n",
            "  inflating: yelp_academic_dataset_review.json  \n",
            "  inflating: yelp_academic_dataset_tip.json  \n",
            "  inflating: yelp_academic_dataset_user.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing of the text:"
      ],
      "metadata": {
        "id": "tKUb6-U3agft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcess(text):\n",
        "  transformedText = text.casefold()\n",
        "  transformedText = transformedText.replace(',','')\n",
        "  transformedText = transformedText.replace('!','')\n",
        "  transformedText = transformedText.replace('?','')\n",
        "  transformedText = transformedText.replace('.','')\n",
        "  transformedText = transformedText.replace('\\n','')\n",
        "  transformedText = transformedText.replace('  ',' ')\n",
        "  return transformedText"
      ],
      "metadata": {
        "id": "LYxRxYATakvJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the text as a list of unique k-shingle stored as hash value of size 4"
      ],
      "metadata": {
        "id": "zWndz5vkR1QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getKShingles(text, k):\n",
        "    global universalSet\n",
        "    shingles=[]\n",
        "    for i in range(0,len(text)-k):\n",
        "      shingle = ''\n",
        "      for j in range (k):\n",
        "        shingle+=text[i+j]\n",
        "      shingle = hashlib.sha1(shingle.encode('utf-8')).hexdigest()[:4]\n",
        "      if shingle not in shingles:\n",
        "        shingles.append(shingle)\n",
        "      if shingle not in universalSet:\n",
        "        universalSet.append(shingle)\n",
        "\n",
        "    return shingles\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lyV-A07t_sPd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a list of k-shingle for every text in the dataset and add every new shingle to the universal set"
      ],
      "metadata": {
        "id": "GTNqQsZASPfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createSets(dataset,k):\n",
        "  sets =[]\n",
        "  for i in range(len(dataset)):\n",
        "    text = preProcess(dataset.get('text')[i])\n",
        "    mySet= getKShingles(text,k)\n",
        "    sets.append(mySet)\n",
        "  return sets\n"
      ],
      "metadata": {
        "id": "puouC4WtRoBW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list which contains the n first natural number in a random order"
      ],
      "metadata": {
        "id": "SlPpsPjRShjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomPermutation(n):\n",
        "  permutedOrder = []\n",
        "  for i in range(n):\n",
        "    permutedOrder.append(random.getrandbits(128) %n)\n",
        "  return permutedOrder\n"
      ],
      "metadata": {
        "id": "Zrq3Ji70L2EE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the signature of every set of shingles in sets with respect to a random permutation"
      ],
      "metadata": {
        "id": "RiQYOltoS4Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeSignature(permutation,sets):\n",
        "  signature = []\n",
        "  nb_sets = len(sets)\n",
        "  permutation_size = len(permutation)\n",
        "  for r in range(nb_sets):\n",
        "    signature.append(float('inf'))\n",
        "  for i in range(nb_sets):\n",
        "    for j in range(permutation_size):\n",
        "      if universalSet[permutation[j]] in sets[i] and permutation[j]<signature[i]:\n",
        "        signature[i]=permutation[j]\n",
        "  return signature"
      ],
      "metadata": {
        "id": "yyVW1-BOiJIr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the  signature matrix of our sets with only one permutation of our universal set divided in m subsets (to have m number in our signatures)"
      ],
      "metadata": {
        "id": "v53CKS6wTi0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSignature(sets,m):\n",
        "  nb_subset = m\n",
        "  subsetSize=len(universalSet)//m\n",
        "  M=randomPermutation(len(universalSet))\n",
        "  signatures = []\n",
        "  for j in range(len(sets)):\n",
        "    signatures.append([])\n",
        "\n",
        "  for i in range(nb_subset):\n",
        "    partial_signature = computeSignature(M[subsetSize*i:subsetSize*(i+1)],sets)\n",
        "    for k in range(len(sets)):\n",
        "      signatures[k].append(partial_signature[k])\n",
        "  return signatures\n",
        "\n"
      ],
      "metadata": {
        "id": "BToG3gcIfQ4s"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return bags of similar item if their signatures hash to the same value"
      ],
      "metadata": {
        "id": "bwiIZiQjUIX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSimilarPairs(signatures):\n",
        "  dic={}\n",
        "  ans=[]\n",
        "  for sig in range(len(signatures)):\n",
        "  \n",
        "    hash_value = hash(signatures[sig])\n",
        "    if(hash_value in dic.keys()):\n",
        "      \n",
        "      dic[hash_value].append(sig)\n",
        "    else:\n",
        "      dic[hash_value]=[sig]\n",
        "  for value in dic.values():\n",
        "    if len(value)>1:\n",
        "      ans.append(value)\n",
        "  \n",
        "  return ans\n",
        "\n"
      ],
      "metadata": {
        "id": "N0KVr7rvtRN1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a list of potential pair candidates"
      ],
      "metadata": {
        "id": "I9mAJ9IVrU-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPairs(possible):\n",
        "  pairs=[]\n",
        "  for pair in possible:\n",
        "    for i in range(len(pair)-1):\n",
        "      for j in range(i+1,len(pair)):\n",
        "        candidate = [pair[i],pair[j]]\n",
        "        if candidate not in pairs:\n",
        "          pairs.append(candidate)\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "iWdudIlWMbQt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the number of bands depanding on the threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "-2es8fm0dMU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getBands(t,size):\n",
        "  for b in range(size, 1,-1):\n",
        "    r=size//b\n",
        "    threshold = (1/b)**(1/r)\n",
        "    if(threshold>t):\n",
        "      return b\n"
      ],
      "metadata": {
        "id": "NoVXAHiHdL42"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide our signature matrix in nb_band. Each band are then process throw the getSimilarPairs function to get potential candidate for high similaritie"
      ],
      "metadata": {
        "id": "9CBjDsuCUXjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def divideBand(signatures,nb_band):\n",
        "  r = len(signatures[0])//nb_band\n",
        "  ans=[]\n",
        "  for i in range(nb_band):\n",
        "    band = []\n",
        "    for sig in signatures:\n",
        "      band.append(tuple(sig[i*r:(i+1)*r]))\n",
        "    similar=getSimilarPairs(band)\n",
        "    for sim in similar:\n",
        "      if sim not in ans:\n",
        "        ans.append(sim)\n",
        "  return getPairs(ans)"
      ],
      "metadata": {
        "id": "s2ytWIfJxn0A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the similaritie between two signatures"
      ],
      "metadata": {
        "id": "G-Eeq7APTSv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSimilaritiesWithS(sig1,sig2):\n",
        "  agree=0\n",
        "  for i in range(len(sig1)):\n",
        "    if sig1[i]==sig2[i] and sig1[i]!=float('inf'):\n",
        "      agree+=1\n",
        "  return agree/len(sig1)"
      ],
      "metadata": {
        "id": "2_baNWy6Wnuv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a list of triplet with the first two elemnt of a triplet are the text and the third one, the similaritie. This similaritie must be at least equal to s."
      ],
      "metadata": {
        "id": "tLSSwmOwrk-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findAllSimilarities(couples,signatures,s):\n",
        "  answer = []\n",
        "  for couple in couples:\n",
        "    sim = getSimilaritiesWithS(signatures[couple[0]],signatures[couple[1]])\n",
        "    if sim>s:\n",
        "      answer.append([couple[0],couple[1],sim])\n",
        "  return answer"
      ],
      "metadata": {
        "id": "PRLeVZeS5Tgg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getText(dataset,i):\n",
        "  return dataset['text'][i]"
      ],
      "metadata": {
        "id": "YCcrCwUZKRY9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of our variables"
      ],
      "metadata": {
        "id": "kA4Lc8KIr24C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "universalSet=[]\n",
        "nb_documents=10000\n",
        "k=5\n",
        "threshold=0.6\n",
        "m=100"
      ],
      "metadata": {
        "id": "_0CDK41evkhw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_json('yelp_academic_dataset_review.json',lines=True,nrows=nb_documents)\n",
        "sets=createSets(dataset,k)                    #List of all list of k-shingles representing our documents\n",
        "signatures = getSignature(sets,m)        #Signature matrix for our documents (each line is a signature)\n",
        "nb_band =getBands(threshold,m)\n",
        "possiblePairs= divideBand(signatures,nb_band)   #List of possible pairs in our documents"
      ],
      "metadata": {
        "id": "W3TBmykdGvWU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There are\",len((possiblePairs)),\"potential similar pairs\")\n",
        "similarItems = findAllSimilarities(possiblePairs,signatures,threshold)\n",
        "print(\"There are\", len(similarItems), \"pairs of document with a similaritie above\",threshold)\n",
        "print(similarItems)"
      ],
      "metadata": {
        "id": "QlFo7hmN2Uu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae9c4ef-52ee-4b04-c6b8-6aace61d6d4d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2410 potential similar pairs\n",
            "There are 2 pairs of document with a similaritie above 0.6\n",
            "[[1715, 9839, 0.85], [6523, 7971, 0.73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two must similar comments"
      ],
      "metadata": {
        "id": "WFqOcR6vy_gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(getText(dataset,1715))\n",
        "print('----------------------------------------------')\n",
        "print(getText(dataset,9839))"
      ],
      "metadata": {
        "id": "OJkmUFBMHOqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3997ba4-bb3e-42d3-caad-e8f6cb5253e8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fun place, inviting interior and a pleasure to hang outside with the couches, etc. brunch food was very good, and beer selection exceptional.  Waiter, however, was a disaster.  Was jerky to us from the beginning, (we were all sitting around asking each other why he was being such a jerk to us), did not know the menu at all, constantly disappeared on us, and was argumentative when he brought out the wrong order (twice)...and there were only 2 other tables at the time). Otherwise, would have gotten 4 stars.\n",
            "----------------------------------------------\n",
            "Fun place, inviting interior and a pleasure to hang outside with the couches, etc. brunch food was very good, and beer selection exceptional.  Waiter, however, was a disaster.  Was jerky to us from the beginning (we were all sitting around asking each other why he was being such an asshole to us), did not know the menu at all, constantly disappeared on us, and was argumentative when he brought out the wrong order (twice).  Otherwise, would have gotten 4 stars.\n"
          ]
        }
      ]
    }
  ]
}